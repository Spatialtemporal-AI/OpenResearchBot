"""Automatic training tracker for PyTorch training loops.

åœ¨è®­ç»ƒè„šæœ¬ä¸­åªéœ€åŠ å‡ è¡Œä»£ç ï¼Œå³å¯è‡ªåŠ¨è®°å½•è®­ç»ƒè¿‡ç¨‹åˆ° OpenResearchBot ç³»ç»Ÿï¼Œ
Agent å’Œ Dashboard å¯ä»¥å®æ—¶çœ‹åˆ°è®­ç»ƒè¿›å±•ã€‚

## åŸºæœ¬ç”¨æ³•ï¼ˆPyTorch åŸç”Ÿå¾ªç¯ï¼‰

```python
from nanobot.tracker import NanobotTracker

# è®­ç»ƒå¼€å§‹ â€” è‡ªåŠ¨åˆ›å»ºè®°å½•ã€æ£€æµ‹ GPU
tracker = NanobotTracker(
    name="OpenVLA-7B finetune",
    model="OpenVLA-7B",
    dataset="bridge_v2",
    hyperparams={"lr": 2e-5, "batch_size": 32, "epochs": 100},
)

for epoch in range(100):
    loss = train_one_epoch()
    tracker.log(epoch=epoch, loss=loss)           # è‡ªåŠ¨è®°å½•æŒ‡æ ‡
    tracker.log(epoch=epoch, loss=val_loss, success_rate=0.8)

tracker.finish()  # æ ‡è®°å®Œæˆï¼ˆä¹Ÿå¯ç”¨ with è¯­å¥è‡ªåŠ¨ç®¡ç†ï¼‰
```

## ä½¿ç”¨ with è¯­å¥ï¼ˆæ¨èï¼Œå¼‚å¸¸æ—¶è‡ªåŠ¨æ ‡è®° failedï¼‰

```python
with NanobotTracker(name="my-exp", model="OpenVLA-7B") as tracker:
    for epoch in range(100):
        loss = train_one_epoch()
        tracker.log(epoch=epoch, loss=loss)
    # æ­£å¸¸é€€å‡ºè‡ªåŠ¨æ ‡è®° completed
# å¼‚å¸¸é€€å‡ºè‡ªåŠ¨æ ‡è®° failed
```

## HuggingFace Trainer é›†æˆ

```python
from nanobot.tracker import NanobotHFCallback

trainer = Trainer(
    model=model,
    args=training_args,
    callbacks=[NanobotHFCallback(name="my-exp", model="OpenVLA-7B")],
)
trainer.train()  # è‡ªåŠ¨è®°å½•æ‰€æœ‰ metrics
```

## è‡ªå®šä¹‰ workspace è·¯å¾„

```python
tracker = NanobotTracker(name="exp", workspace="/path/to/workspace")
```

é»˜è®¤è‡ªåŠ¨æŸ¥æ‰¾: ./workspace > ç¯å¢ƒå˜é‡ NANOBOT_WORKSPACE > å½“å‰ç›®å½•
"""

from __future__ import annotations

import atexit
import json
import os
import platform
import sys
import threading
import time
import uuid
from datetime import datetime
from pathlib import Path
from typing import Any


# ---------------------------------------------------------------------------
# Helpers
# ---------------------------------------------------------------------------

def _now_iso() -> str:
    return datetime.now().strftime("%Y-%m-%dT%H:%M:%S")


def _short_id() -> str:
    return "run-" + uuid.uuid4().hex[:6]


def _detect_gpu_info() -> str:
    """å°è¯•è‡ªåŠ¨æ£€æµ‹ GPU ä¿¡æ¯ã€‚"""
    try:
        import torch
        if torch.cuda.is_available():
            count = torch.cuda.device_count()
            name = torch.cuda.get_device_name(0)
            mem = torch.cuda.get_device_properties(0).total_mem
            mem_gb = mem / (1024 ** 3)
            if count > 1:
                return f"{count}x {name} ({mem_gb:.0f}GB)"
            return f"{name} ({mem_gb:.0f}GB)"
    except Exception:
        pass

    # Fallback: try nvidia-smi
    try:
        import subprocess
        result = subprocess.run(
            ["nvidia-smi", "--query-gpu=name,memory.total", "--format=csv,noheader,nounits"],
            capture_output=True, text=True, timeout=5,
        )
        if result.returncode == 0:
            lines = [l.strip() for l in result.stdout.strip().split("\n") if l.strip()]
            if lines:
                count = len(lines)
                name, mem = lines[0].split(",", 1)
                mem_gb = int(mem.strip()) / 1024
                if count > 1:
                    return f"{count}x {name.strip()} ({mem_gb:.0f}GB)"
                return f"{name.strip()} ({mem_gb:.0f}GB)"
    except Exception:
        pass

    return ""


def _detect_script_name() -> str:
    """è·å–å½“å‰è¿è¡Œè„šæœ¬çš„åç§°ã€‚"""
    try:
        main = sys.modules.get("__main__")
        if main and hasattr(main, "__file__") and main.__file__:
            return Path(main.__file__).stem
    except Exception:
        pass
    return ""


def _find_workspace(explicit: str | Path | None = None) -> Path:
    """æŸ¥æ‰¾ workspace ç›®å½•ã€‚ä¼˜å…ˆçº§: æ˜¾å¼æŒ‡å®š > ç¯å¢ƒå˜é‡ > ./workspace > å½“å‰ç›®å½•"""
    if explicit:
        p = Path(explicit)
        p.mkdir(parents=True, exist_ok=True)
        return p

    env = os.environ.get("NANOBOT_WORKSPACE")
    if env:
        p = Path(env)
        p.mkdir(parents=True, exist_ok=True)
        return p

    # å‘ä¸ŠæŸ¥æ‰¾ workspace ç›®å½•ï¼ˆæœ€å¤š 5 å±‚ï¼‰
    cwd = Path.cwd()
    for parent in [cwd] + list(cwd.parents)[:5]:
        wp = parent / "workspace"
        if wp.exists() and wp.is_dir():
            return wp

    # æœ€ç»ˆ fallbackï¼šåœ¨å½“å‰ç›®å½•åˆ›å»º workspace
    wp = cwd / "workspace"
    wp.mkdir(parents=True, exist_ok=True)
    return wp


# ---------------------------------------------------------------------------
# Storage (ä¸ TrainingTrackerTool å®Œå…¨å…¼å®¹çš„ JSON å­˜å‚¨)
# ---------------------------------------------------------------------------

class _Storage:
    """çº¿ç¨‹å®‰å…¨çš„ JSON å­˜å‚¨ï¼Œä¸ TrainingTrackerTool å…±äº«åŒä¸€ä¸ªæ–‡ä»¶ã€‚"""

    def __init__(self, workspace: Path):
        self._path = workspace / "research" / "training_runs.json"
        self._path.parent.mkdir(parents=True, exist_ok=True)
        self._lock = threading.Lock()

    def load(self) -> list[dict]:
        with self._lock:
            if self._path.exists():
                try:
                    data = json.loads(self._path.read_text(encoding="utf-8"))
                    return data.get("runs", [])
                except (json.JSONDecodeError, KeyError):
                    return []
            return []

    def save(self, runs: list[dict]) -> None:
        with self._lock:
            self._path.parent.mkdir(parents=True, exist_ok=True)
            payload = {"version": 1, "runs": runs}
            # å†™å…¥ä¸´æ—¶æ–‡ä»¶å†é‡å‘½åï¼Œé˜²æ­¢å†™å…¥ä¸­é€”è¢«è¯»å–åˆ°ä¸å®Œæ•´æ•°æ®
            tmp = self._path.with_suffix(".tmp")
            tmp.write_text(
                json.dumps(payload, indent=2, ensure_ascii=False),
                encoding="utf-8",
            )
            tmp.replace(self._path)

    def update_run(self, run_id: str, updater: Any) -> dict | None:
        """è¯»å– â†’ æ‰¾åˆ° run â†’ è°ƒç”¨ updater(run) â†’ ä¿å­˜ã€‚è¿”å›æ›´æ–°åçš„ runã€‚"""
        runs = self.load()
        run = next((r for r in runs if r["id"] == run_id), None)
        if run is None:
            return None
        updater(run)
        self.save(runs)
        return run


# ---------------------------------------------------------------------------
# NanobotTracker â€” PyTorch è®­ç»ƒè‡ªåŠ¨è®°å½•å™¨
# ---------------------------------------------------------------------------

class NanobotTracker:
    """PyTorch è®­ç»ƒè‡ªåŠ¨è®°å½•å™¨ã€‚

    åœ¨è®­ç»ƒè„šæœ¬ä¸­åµŒå…¥å‡ è¡Œä»£ç ï¼Œè‡ªåŠ¨è®°å½•è®­ç»ƒè¿‡ç¨‹ã€‚
    æ•°æ®ç›´æ¥å†™å…¥ workspace/research/training_runs.jsonï¼Œ
    Agent å’Œ Dashboard å¯ä»¥å®æ—¶çœ‹åˆ°ã€‚

    Features:
    - è‡ªåŠ¨æ£€æµ‹ GPU ä¿¡æ¯
    - è‡ªåŠ¨æ£€æµ‹è„šæœ¬åç§°
    - with è¯­å¥æ”¯æŒï¼ˆæ­£å¸¸é€€å‡º â†’ completedï¼Œå¼‚å¸¸ â†’ failedï¼‰
    - atexit å…œåº•ï¼ˆè¿›ç¨‹è¢« kill æ—¶æ ‡è®° stoppedï¼‰
    - å¯æ§æ—¥å¿—é¢‘ç‡ï¼ˆlog_every_n_stepsï¼‰
    - çº¿ç¨‹å®‰å…¨
    - checkpoint è®°å½•
    """

    def __init__(
        self,
        name: str,
        model: str = "",
        dataset: str = "",
        hyperparams: dict | None = None,
        gpu_info: str = "",
        vla_config: dict | None = None,
        note: str | None = None,
        workspace: str | Path | None = None,
        log_every_n_steps: int = 1,
        auto_detect_gpu: bool = True,
    ):
        """
        Args:
            name: è®­ç»ƒè¿è¡Œçš„åç§°ï¼ˆå¿…å¡«ï¼‰
            model: æ¨¡å‹åç§°/æ¶æ„
            dataset: è®­ç»ƒæ•°æ®é›†
            hyperparams: è¶…å‚æ•°å­—å…¸
            gpu_info: GPU ä¿¡æ¯ï¼ˆç•™ç©ºåˆ™è‡ªåŠ¨æ£€æµ‹ï¼‰
            vla_config: VLA ç‰¹æœ‰é…ç½®
            note: åˆå§‹å¤‡æ³¨
            workspace: workspace è·¯å¾„ï¼ˆç•™ç©ºåˆ™è‡ªåŠ¨æŸ¥æ‰¾ï¼‰
            log_every_n_steps: æ¯ N æ¬¡è°ƒç”¨ log() æ‰å®é™…å†™å…¥ç£ç›˜ï¼ˆé»˜è®¤1ï¼Œå³æ¯æ¬¡éƒ½å†™ï¼‰
            auto_detect_gpu: æ˜¯å¦è‡ªåŠ¨æ£€æµ‹ GPU ä¿¡æ¯
        """
        self._workspace = _find_workspace(workspace)
        self._storage = _Storage(self._workspace)
        self._log_every = max(1, log_every_n_steps)
        self._log_count = 0
        self._pending_metrics: dict | None = None  # ç¼“å†²åŒº
        self._finished = False

        # è‡ªåŠ¨æ£€æµ‹ GPU
        if not gpu_info and auto_detect_gpu:
            gpu_info = _detect_gpu_info()

        # è‡ªåŠ¨è¡¥å……åç§°
        if not name:
            name = _detect_script_name() or f"training-{_short_id()}"

        # è‡ªåŠ¨ä» hyperparams æå–å¸¸ç”¨å­—æ®µ
        hp = hyperparams or {}

        # åˆ›å»ºè®­ç»ƒè®°å½•
        now = _now_iso()
        self._run_id = _short_id()
        run: dict[str, Any] = {
            "id": self._run_id,
            "name": name,
            "model": model,
            "dataset": dataset,
            "hyperparams": hp,
            "status": "running",
            "gpu_info": gpu_info,
            "vla_config": vla_config or {},
            "metrics_history": [],
            "latest_metrics": {},
            "notes": [],
            "checkpoints": [],
            "created": now,
            "updated": now,
            "started": now,
            "finished": None,
            # é¢å¤–å…ƒæ•°æ®
            "_meta": {
                "script": _detect_script_name(),
                "hostname": platform.node(),
                "python": platform.python_version(),
                "pid": os.getpid(),
                "auto_tracked": True,
            },
        }

        if note:
            run["notes"].append({"time": now, "content": note})

        # ä¿å­˜
        runs = self._storage.load()
        runs.append(run)
        self._storage.save(runs)

        # æ³¨å†Œ atexitï¼Œè¿›ç¨‹æ„å¤–é€€å‡ºæ—¶æ ‡è®° stopped
        atexit.register(self._atexit_hook)

        self._print(
            f"ğŸš€ NanobotTracker: è®­ç»ƒå·²æ³¨å†Œ [{self._run_id}] {name}"
            + (f" | model={model}" if model else "")
            + (f" | gpu={gpu_info}" if gpu_info else "")
        )

    # ---------- å…¬å¼€ API ----------

    @property
    def run_id(self) -> str:
        """å½“å‰è®­ç»ƒè¿è¡Œçš„ IDã€‚"""
        return self._run_id

    @property
    def workspace(self) -> Path:
        return self._workspace

    def log(self, **metrics: Any) -> None:
        """è®°å½•è®­ç»ƒæŒ‡æ ‡ã€‚

        Examples:
            tracker.log(epoch=1, loss=0.5, lr=1e-4)
            tracker.log(step=1000, loss=0.3, success_rate=0.67)
        """
        if self._finished:
            return

        self._log_count += 1
        self._pending_metrics = metrics

        # æŒ‰é¢‘ç‡æ§åˆ¶å†™å…¥
        if self._log_count % self._log_every == 0:
            self._flush_metrics()

    def log_checkpoint(self, path: str, **extra_metrics: Any) -> None:
        """è®°å½•ä¸€ä¸ª checkpointã€‚

        Args:
            path: checkpoint è·¯å¾„æˆ–åç§°
            **extra_metrics: å¯é€‰çš„é¢å¤–æŒ‡æ ‡
        """
        if self._finished:
            return

        if extra_metrics:
            self.log(**extra_metrics)

        def _update(run: dict):
            run.setdefault("checkpoints", []).append(path)
            run["updated"] = _now_iso()

        self._storage.update_run(self._run_id, _update)
        self._print(f"ğŸ’¾ Checkpoint: {path}")

    def add_note(self, content: str) -> None:
        """æ·»åŠ ä¸€æ¡å¤‡æ³¨ã€‚"""
        if self._finished:
            return

        now = _now_iso()

        def _update(run: dict):
            run.setdefault("notes", []).append({"time": now, "content": content})
            run["updated"] = now

        self._storage.update_run(self._run_id, _update)

    def update_hyperparams(self, **kwargs: Any) -> None:
        """è¿½åŠ /æ›´æ–°è¶…å‚æ•°ã€‚"""
        def _update(run: dict):
            run.setdefault("hyperparams", {}).update(kwargs)
            run["updated"] = _now_iso()

        self._storage.update_run(self._run_id, _update)

    def update_vla_config(self, **kwargs: Any) -> None:
        """è¿½åŠ /æ›´æ–° VLA é…ç½®ã€‚"""
        def _update(run: dict):
            run.setdefault("vla_config", {}).update(kwargs)
            run["updated"] = _now_iso()

        self._storage.update_run(self._run_id, _update)

    def finish(self, note: str | None = None) -> None:
        """æ ‡è®°è®­ç»ƒå®Œæˆã€‚"""
        self._set_status("completed", note)

    def fail(self, note: str | None = None) -> None:
        """æ ‡è®°è®­ç»ƒå¤±è´¥ã€‚"""
        self._set_status("failed", note)

    def stop(self, note: str | None = None) -> None:
        """æ ‡è®°è®­ç»ƒæ‰‹åŠ¨åœæ­¢ã€‚"""
        self._set_status("stopped", note)

    # ---------- Context manager ----------

    def __enter__(self) -> "NanobotTracker":
        return self

    def __exit__(self, exc_type, exc_val, exc_tb) -> None:
        if self._finished:
            return
        if exc_type is not None:
            # å¼‚å¸¸é€€å‡º â†’ failed
            err_msg = f"{exc_type.__name__}: {exc_val}" if exc_val else str(exc_type.__name__)
            self.fail(note=f"å¼‚å¸¸é€€å‡º: {err_msg}")
        else:
            # æ­£å¸¸é€€å‡º â†’ completed
            self.finish()
        return None  # ä¸æŠ‘åˆ¶å¼‚å¸¸

    # ---------- å†…éƒ¨æ–¹æ³• ----------

    def _flush_metrics(self) -> None:
        """å°†ç¼“å†²çš„æŒ‡æ ‡å†™å…¥ç£ç›˜ã€‚"""
        metrics = self._pending_metrics
        if not metrics:
            return
        self._pending_metrics = None
        now = _now_iso()
        entry = {"time": now, **metrics}

        def _update(run: dict):
            run.setdefault("metrics_history", []).append(entry)
            run.setdefault("latest_metrics", {}).update(metrics)
            run["updated"] = now
            # å¦‚æœè¿˜æ˜¯ queuedï¼Œè‡ªåŠ¨æ”¹æˆ running
            if run.get("status") == "queued":
                run["status"] = "running"
                if not run.get("started"):
                    run["started"] = now

        self._storage.update_run(self._run_id, _update)

        # è¾“å‡ºè¿›åº¦æ‘˜è¦
        parts = []
        for k in ["epoch", "step", "loss", "success_rate", "lr", "eval_loss"]:
            if k in metrics:
                v = metrics[k]
                if isinstance(v, float) and k != "epoch":
                    parts.append(f"{k}={v:.4g}")
                else:
                    parts.append(f"{k}={v}")
        # è¡¥å……å…¶ä½™å­—æ®µ
        for k, v in metrics.items():
            if k not in ["epoch", "step", "loss", "success_rate", "lr", "eval_loss"]:
                if isinstance(v, float):
                    parts.append(f"{k}={v:.4g}")
                else:
                    parts.append(f"{k}={v}")
        if parts:
            self._print(f"ğŸ“ˆ [{self._run_id}] {', '.join(parts)}")

    def _set_status(self, status: str, note: str | None = None) -> None:
        """è®¾ç½®è®­ç»ƒçŠ¶æ€å¹¶åˆ·æ–°ç¼“å†²ã€‚"""
        if self._finished:
            return
        self._finished = True

        # å…ˆåˆ·æ–°æœªå†™å…¥çš„æŒ‡æ ‡
        if self._pending_metrics:
            self._flush_metrics()

        now = _now_iso()

        def _update(run: dict):
            run["status"] = status
            run["finished"] = now
            run["updated"] = now
            if note:
                run.setdefault("notes", []).append({"time": now, "content": note})

        self._storage.update_run(self._run_id, _update)

        emoji = {"completed": "âœ…", "failed": "âŒ", "stopped": "â¹ï¸"}.get(status, "â“")
        self._print(f"{emoji} [{self._run_id}] è®­ç»ƒ{status}")

    def _atexit_hook(self) -> None:
        """è¿›ç¨‹é€€å‡ºæ—¶çš„å…œåº•å¤„ç†ã€‚"""
        if not self._finished:
            # åˆ·æ–°ç¼“å†²æŒ‡æ ‡
            if self._pending_metrics:
                self._flush_metrics()
            self._set_status("stopped", note="è¿›ç¨‹é€€å‡º (atexit)")

    @staticmethod
    def _print(msg: str) -> None:
        """å®‰å…¨æ‰“å°ï¼ˆå…¼å®¹ Windows GBK ç»ˆç«¯ï¼‰ã€‚"""
        try:
            print(msg, flush=True)
        except UnicodeEncodeError:
            print(
                msg.encode(sys.stdout.encoding or "utf-8", errors="replace")
                .decode(sys.stdout.encoding or "utf-8", errors="replace"),
                flush=True,
            )


# ---------------------------------------------------------------------------
# NanobotHFCallback â€” HuggingFace Trainer å›è°ƒ
# ---------------------------------------------------------------------------

class NanobotHFCallback:
    """HuggingFace Transformers Trainer è‡ªåŠ¨è®°å½•å›è°ƒã€‚

    Usage:
        from nanobot.tracker import NanobotHFCallback

        trainer = Trainer(
            model=model,
            args=training_args,
            callbacks=[NanobotHFCallback(name="my-exp", model="OpenVLA-7B")],
        )
        trainer.train()
    """

    def __init__(
        self,
        name: str = "",
        model: str = "",
        dataset: str = "",
        hyperparams: dict | None = None,
        gpu_info: str = "",
        vla_config: dict | None = None,
        workspace: str | Path | None = None,
        log_every_n_steps: int = 1,
    ):
        self._init_kwargs = {
            "name": name,
            "model": model,
            "dataset": dataset,
            "hyperparams": hyperparams,
            "gpu_info": gpu_info,
            "vla_config": vla_config,
            "workspace": workspace,
            "log_every_n_steps": log_every_n_steps,
        }
        self._tracker: NanobotTracker | None = None

    def on_train_begin(self, args, state, control, **kwargs):
        """è®­ç»ƒå¼€å§‹æ—¶è‡ªåŠ¨åˆ›å»ºè®°å½•ã€‚"""
        init = self._init_kwargs.copy()

        # ä» TrainingArguments æå–è¶…å‚æ•°
        hp = init.get("hyperparams") or {}
        if hasattr(args, "learning_rate"):
            hp.setdefault("lr", args.learning_rate)
        if hasattr(args, "per_device_train_batch_size"):
            hp.setdefault("batch_size", args.per_device_train_batch_size)
        if hasattr(args, "num_train_epochs"):
            hp.setdefault("epochs", int(args.num_train_epochs))
        if hasattr(args, "weight_decay"):
            hp.setdefault("weight_decay", args.weight_decay)
        if hasattr(args, "warmup_steps"):
            hp.setdefault("warmup_steps", args.warmup_steps)
        if hasattr(args, "gradient_accumulation_steps"):
            hp.setdefault("grad_accum", args.gradient_accumulation_steps)
        init["hyperparams"] = hp

        # ä» model kwargs æå–æ¨¡å‹åç§°
        if not init["name"]:
            init["name"] = _detect_script_name() or "hf-training"
        if not init["model"] and "model" in kwargs:
            m = kwargs["model"]
            if hasattr(m, "config") and hasattr(m.config, "_name_or_path"):
                init["model"] = m.config._name_or_path

        self._tracker = NanobotTracker(**init)

    def on_log(self, args, state, control, logs=None, **kwargs):
        """æ¯æ¬¡ Trainer æ‰“å°æ—¥å¿—æ—¶è‡ªåŠ¨è®°å½•æŒ‡æ ‡ã€‚"""
        if self._tracker is None or logs is None:
            return

        # æå–æœ‰ç”¨çš„æŒ‡æ ‡
        metrics = {}
        for key in ["loss", "eval_loss", "learning_rate", "epoch",
                     "grad_norm", "eval_accuracy", "eval_f1",
                     "eval_precision", "eval_recall"]:
            if key in logs:
                metrics[key] = logs[key]

        # è®°å½• step
        if state and hasattr(state, "global_step"):
            metrics["step"] = state.global_step

        # ä¹Ÿè®°å½•å…¶ä»– eval_ å¼€å¤´çš„æŒ‡æ ‡
        for k, v in logs.items():
            if k.startswith("eval_") and k not in metrics:
                metrics[k] = v

        if metrics:
            self._tracker.log(**metrics)

    def on_save(self, args, state, control, **kwargs):
        """ä¿å­˜ checkpoint æ—¶è‡ªåŠ¨è®°å½•ã€‚"""
        if self._tracker is None:
            return
        if state and hasattr(state, "best_model_checkpoint") and state.best_model_checkpoint:
            self._tracker.log_checkpoint(state.best_model_checkpoint)
        elif args and hasattr(args, "output_dir"):
            step = state.global_step if state else "unknown"
            self._tracker.log_checkpoint(f"{args.output_dir}/checkpoint-{step}")

    def on_train_end(self, args, state, control, **kwargs):
        """è®­ç»ƒç»“æŸæ—¶è‡ªåŠ¨æ ‡è®°å®Œæˆã€‚"""
        if self._tracker is None:
            return
        self._tracker.finish()

    def on_evaluate(self, args, state, control, metrics=None, **kwargs):
        """è¯„ä¼°ç»“æŸæ—¶è®°å½•è¯„ä¼°æŒ‡æ ‡ã€‚"""
        if self._tracker is None or metrics is None:
            return
        eval_metrics = {}
        for k, v in metrics.items():
            if isinstance(v, (int, float)):
                eval_metrics[k] = v
        if eval_metrics:
            self._tracker.log(**eval_metrics)


# ---------------------------------------------------------------------------
# PyTorch Lightning Callback (å¯é€‰)
# ---------------------------------------------------------------------------

class NanobotLightningCallback:
    """PyTorch Lightning è‡ªåŠ¨è®°å½•å›è°ƒã€‚

    Usage:
        from nanobot.tracker import NanobotLightningCallback

        trainer = pl.Trainer(
            callbacks=[NanobotLightningCallback(name="my-exp", model="OpenVLA-7B")],
        )
    """

    def __init__(
        self,
        name: str = "",
        model: str = "",
        dataset: str = "",
        hyperparams: dict | None = None,
        workspace: str | Path | None = None,
        log_every_n_steps: int = 1,
    ):
        self._init_kwargs = {
            "name": name or _detect_script_name() or "lightning-training",
            "model": model,
            "dataset": dataset,
            "hyperparams": hyperparams,
            "workspace": workspace,
            "log_every_n_steps": log_every_n_steps,
        }
        self._tracker: NanobotTracker | None = None

    def on_train_start(self, trainer, pl_module):
        hp = self._init_kwargs.get("hyperparams") or {}
        if hasattr(trainer, "max_epochs"):
            hp.setdefault("epochs", trainer.max_epochs)
        if hasattr(pl_module, "learning_rate"):
            hp.setdefault("lr", pl_module.learning_rate)
        self._init_kwargs["hyperparams"] = hp
        self._tracker = NanobotTracker(**self._init_kwargs)

    def on_train_epoch_end(self, trainer, pl_module):
        if self._tracker is None:
            return
        metrics = {"epoch": trainer.current_epoch}
        # ä» trainer.callback_metrics è·å–
        for k, v in trainer.callback_metrics.items():
            try:
                metrics[k] = float(v)
            except (TypeError, ValueError):
                pass
        self._tracker.log(**metrics)

    def on_train_end(self, trainer, pl_module):
        if self._tracker:
            self._tracker.finish()


# ---------------------------------------------------------------------------
# ä¾¿æ·å‡½æ•°
# ---------------------------------------------------------------------------

def track_training(
    name: str,
    model: str = "",
    dataset: str = "",
    hyperparams: dict | None = None,
    **kwargs: Any,
) -> NanobotTracker:
    """ä¾¿æ·å‡½æ•°ï¼Œç­‰åŒäº NanobotTracker(...)ã€‚

    Example:
        tracker = track_training("my-exp", model="OpenVLA-7B", hyperparams={"lr": 2e-5})
    """
    return NanobotTracker(
        name=name, model=model, dataset=dataset,
        hyperparams=hyperparams, **kwargs,
    )
